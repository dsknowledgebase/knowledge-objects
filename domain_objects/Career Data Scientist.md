# A Learning guide for a Career Data Scientist
Author : Pranav Gujarathi ([Email](pgujarat@iu.edu)|[Github](www.gitub.com/pranavdg1997)|[LinkedIn](www.linkedin.com/in/pranav-gujarathi))

### Who is this meant for?
This guide is meant for any Data Science enthusiast to start their journey in this field. Specifically, this is meant for those who don't just wish to use Data Science for a particular application, but rather who want to study the subject itself and build a wholesome career in it. All views expressed are mine alone and subjective.\
It is important to note that being an enthusiast and being an aspirational career Data Scientist are two different things, the latter means you have chosen this as your priority, in learning and future career paths, preferrably exclusively. 

### What is unique about Data Science?
This subject, unlike established streams is a new and emerging one, although foundations and building blocks have been around for some time. A Data Scientist needs to be a master of fundamental principles, yet someone who is well read about cutting edge research and can turn algorithms from newly published journals to a production level code. A Data Scientist needs to, as the need presents, put on the hat of a Software Engineer, a Researcher, a Consultant or a Statistician, and sometimes even more than one. While there are no decisive set of programming tools, packages, concepts and topics that can be deemed as necessary or sufficient to be Data Scientist, if one were to make such a list it would change once every six months. \
Yes there is no 'pre-requisite' as such in terms of knowing any programming and mathematics beyond high school level. However, those two things are a big part of a Data Scientist, so feel free to use this statement as a stopping point if you are attracted purely by the $ signs but do not stand easy with the nerdy side of it all. \

### Anything that should be known before starting?
The constantly evolving nature of the subject brings to focus the importance of the ability of self-learning, and how it doesn't matter what stage in life you are with regards to your career. This means it doesn't matter if you are the head of technical R&D at Google or a college sophomore, if a new technique/tool/concept in this field pops up, you should know about it - consider it analogous to a stock broker reading the bsuiness section everyday, there must a periodic attempt to keep up with the advances and new research. No matter how much of a fancy degree you have, the content is not going to age well and a safe bet to assume a lot of it will be obsolete in 5 years - where it is a grad level course from Stanford or . (it does not mean that it doesn't need to be studied, but that just by itself these set of techniques will be not worth much and certainly not sufficient to secure good employment in this field.)\
Now that you have mentally prepared yourself for a career as a Data Scientist, it is time to dive into the learning journey itself. Although the points mention Year, note that this the maximum time you should allot to these learning, and it is possible to cover them in less time too, though not too less.

## Year 1 : From Linear Regression to XGBoost
 - First, to get a feel of what kind of work and problems Data Scientists solve, the [Kaggle Titanic challenge](https://www.kaggle.com/alexisbcook/titanic-tutorial) and accompanying tutorial is an excellent way to start. Don't worry, this has absolutely no pre-requisites and involves close to no programming. It is important to keep in mind the fundamentals of how datasets look, how to get an light overview and what sort of challenges/roadblocks stand in your way.
 - Although Machine Learning is not all that there is in Data Science, a substantial level of expertise both in the mathematical principles and implementation are necessary. [Andre NG's Machine Learning course](https://www.youtube.com/watch?v=PPLop4L2eGk&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&ab_channel=ArtificialIntelligence-AllinOne) is considered a very standard learning resource, and covers fundamentals very well. The programming assignments can be skipped since they use deprecated tools.
 - Now that some theory has been covered, choose a language between [Python](https://github.com/dsknowledgebase/v0/blob/main/knowledge_objects/Python.md) and [R](https://github.com/dsknowledgebase/v0/blob/main/knowledge_objects/R.md) and undergo some basic exploration of Data Manipulation and Predictive Algorithms. I would advice Python because it is has a considerably larger community, even outside the DS world( Larger community means you''re more likely to find help by googling or on social media). Moreover, all the most cutting edge libraries and frameworks are made available for Python by the DS research community. R is easier to learn, however lags behind Python in recieving libraries and packages that utilize cutting edge tools. It is a statistical library, good for typical and simplistic analysis, but not suited for complex, large and industry-compatible tasks. Eventually, a good Data Scientist should reach a level proficiency in programming, where transition to another language in a tight schedule is not a large hassle.
 - For Python, basics of [Numpy](https://github.com/dsknowledgebase/v0/blob/main/knowledge_objects/Numpy.md), [Pandas](https://github.com/dsknowledgebase/v0/blob/main/knowledge_objects/Pandas.md), and Scikit-Learn should be covered, while in R dplyr and h2o libraires should be explored. Nobody tries to learn syntax by heart, and nobody codes without looking at the documentation every now and then, but a level of comfort and ease in writing large pieces of code is what we are looking for. 
 - After a level of programming practice, move on to predictive analytics algorithms and their implementation. Methods like Linear & Logistic Regression, KNN, SVM, K-Means, Decision Trees, Random Forests, XGBoost, their implementation ,etc should be practised on datasets. There should be level of theoretical clarity, where if asked, one can implement any of these from scratch just using basic libraries like Numpy. In fact, implementing few or all of these is an excellent way of knowing one's mastery of programming and whether or not you are ready for Year 2. 
 - One should start being active on Data Science and Predicitve analytics related social media groups and pages. I list this because these are amazing avenues to solve doubts and ask questions when one feels stuck, and also learn about new tools and techniques. Basics covered in this stage are essential, but by no means sufficient, and one should be aware of the gaps between self-knowledge and state-of-the-art and industry best practises.
 - After some of these learnings, it is important to assess and have that mental discussion about your level of commitment going ahead. There are a lot of people who enter or transition into this field with no computer science or even technical background at all, so at times the 'math and programming of it all' can be overwhelming. Like mentioned earlier, due to the constantly changing nature of the field, you won't find all the resources in one neatly organized book or college course, and you will just have to Google a lot until you piece it all together. This is an essential skill in itself, and will be large portion of what you do going ahead.


## Year 2: Competitions, Review and shallow dive into advanced concepts
 - After having worked on some basic 'tabular' datasets, DS and Predictive Modeling competitions are an excellent way of improving skills and knowing the tips and tricks behind going from decent performance to Rank 1. Kaggle, Analytics Vidhya, Hackerearth are some of the places for this.
 - Apart from typical datasets, one must also look into applications in Time Series, Recommendations Systems and Anomaly Detection. Each of these, while they can be tackled by the previously learned techniques, have their own nuances and specific algorithms that work. These include Collaborative Filtering methods, ARIMA, Markov Chains, some Bayesian models, etc. One dataset of each should be worked upon for about a month.
 - Other concepts to look into include Hyper Parameter tuning, Interpretation techniques, basic Natural Language Processing and Visualization libraries like Plotly and GGPlot.
 - One should start maintaining a well documented Github page and writing codes that are elegant, commented and modularized. To get a feel of what we mean by these terms, simply take a look at established libraries and how their codes are maintained. A Github page and your code is a great way of adding 'authenticity' to your skills when looking for employment.
 - Skills covered accross these two years should be sufficient to apply for certain internships and entry level positions, and one should not hesitate to get some practical experience. However, it does not mean the learning is complete or has to be put on hold, the practical experience should be to test these skills and develop an aptitude for advanced concepts. Undertaking long term projects, with a defined problem statement and deliverables are an excellent substitue to industry experience.
 - One should build a familiarity with Big Data concepts - Databases, deploying codes on distributed systems and cloud platforms. 
 - One should check and study the non-technical aspects of Data Science, these include inculcating some professional writing skills, how to prepare presentations and project proposals, explaining technical challenges to non-technical stakeholders, etc.


## Year 3: 'Deep' Dive.
 - This is the year when a deep dive is taken into both the technical and practical parts of Data Science. A knowledge of Linear Algebra and Matrix Manipulations is good place to start, and [Part I](https://www.deeplearningbook.org/) of the Deep Learning Free Online book is a good place to start.
 - A shallow dive into basics of Neural Networks, how they work and their implementation can be studies from [Course 1](https://www.youtube.com/watch?v=CS4cs9xVecg&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&ab_channel=Deeplearning.ai) of DeepLearning.ai specialization or Part II of the Deep Learning Book. This can be followed up with studying topics such as CNNs, LSTMs, Autoencoders, GANs and Transformer models. 
 - Deeper Dive into Deep Learning Application areas, like Computer Vision and Natural Language Processing is recommended. While CV has limited use in industry compared to NLP, the future is open and the value of both the fields is increasing, as their perception as purely research fields is dwindling. Topics for CV include Image Classification, CNNs, Object Detection/ Segmentation problems, while topics for NLP include Word Representations, Transformer models, Document summarization, Q-A models, Topic Modeling, etc.
 - Apart from Deep Learning, one should be master of of handling tabular data, including the use of Databases, Spark/Hadoop/SQL whatever the state-of-art is when you read this. One shuld not rush into learning every tool and syntax for Big Data, rather create an aptitude so that anything can be picked up seamlessly by reading the documentation. One should have enough maturity in conventional analytics to estimate what level of resources are required for what volume of data, an intuition of what algorithms works best keeping in mind pheasibility and the needs of the client.
 - Improve upon GitHub contributions, write cleaner documentations and build a portfolio around your work and skillsets. Make sure your codes are reusable for other datasets and your results are reproducible. This means random people coming accross your repository should be able to understand the codes based on documentations, and reutilize them for their own projects. Experience and ability of writing codes of this calibre is a much valued skill in the industry.
 - Apart from popular models and applications, one should also build a temperament to read research and inculcate newly published algorithms into work. Research papers can be full of formulas and confusing mathematical terminology, however, that is not a reason of turning away from them. By the time someone makes a 'sexy' looking illustration of an algorithm and dumbs it down in a video tutorial or a Blog, it is already old news and you have lost your advantage by waiting for that to happen. Additionally, one should be free of dependency on these formats of learning, they are like training wheels, they make it easier to learn but aren't the real thing. Even if one does not understand everything in a paper, consider revisiting it in a couple months and increasing your understanding. It is also a good way of knowing what all you learnt in those 2 months.


## Year 4: ???
